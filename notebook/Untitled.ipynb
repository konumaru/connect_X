{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://www.kaggle.com/alexisbcook/one-step-lookahead\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import base64\n",
    "import random\n",
    "import pickle\n",
    "import inspect\n",
    "from tqdm import tqdm\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import kaggle_environments as kaggle_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''Preprocessing.\n",
    "'''\n",
    "\n",
    "def preprocess(obs, col_num, row_num):\n",
    "    # 状態は自分のチェッカーを１、相手のチェッカーを0.5とした7*6次元の配列で表現する\n",
    "    state = np.array(obs.board)\n",
    "    state = state.reshape([col_num, row_num])\n",
    "\n",
    "    if obs.mark == 1:\n",
    "        return np.where(state == 2, 0.5, state)\n",
    "    else:\n",
    "        result = np.where(state == 2, 1, state)\n",
    "        return np.where(state == 1, 0.5, state)\n",
    "\n",
    "\n",
    "'''Train Agent.\n",
    "'''\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, output):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.fc = nn.Linear(192, 32)\n",
    "        self.head = nn.Linear(32, output)\n",
    "\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepQNetworkAgent():\n",
    "    def __init__(self, env, lr=0.01, min_experiences=100, max_experiences=10_000, channel=1):\n",
    "        self.env = env\n",
    "        self.model = CNN(output=7)\n",
    "        self.teacher_model = CNN(output=7)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterrion = nn.MSELoss()\n",
    "        self.experience = {'s': [], 'a': [], 'r': [], 'n_s': [], 'done': []}\n",
    "        self.min_experiences = min_experiences\n",
    "        self.max_experiences = max_experiences\n",
    "        self.actions = list(range(self.env.configuration.columns))\n",
    "        self.col_num = self.env.configuration.columns\n",
    "        self.row_num = self.env.configuration.rows\n",
    "        self.channel = channel\n",
    "\n",
    "    def add_experience(self, exp):\n",
    "        # 行動履歴の更新\n",
    "        if len(self.experience['s']) >= self.max_experiences:\n",
    "            # 行動履歴のサイズが大きすぎる時は古いものを削除\n",
    "            for key in self.experience.keys():\n",
    "                self.experience[key].pop(0)\n",
    "\n",
    "        for key, value in exp.items():\n",
    "            self.experience[key].append(value)\n",
    "\n",
    "    def preprocess(self, state):\n",
    "        # 状態は自分のチェッカーを１、相手のチェッカーを0.5とした7*6次元の配列で表現する\n",
    "        result = np.array(state.board[:])\n",
    "        result = result.reshape([self.col_num, self.row_num])\n",
    "\n",
    "        if state.mark == 1:\n",
    "            return np.where(result == 2, 0.5, result)\n",
    "        else:\n",
    "            result = np.where(result == 2, 1, result)\n",
    "            return np.where(result == 1, 0.5, result)\n",
    "\n",
    "    def estimate(self, state):\n",
    "        # 価値の計算\n",
    "        return self.model(\n",
    "            torch.from_numpy(state).view(-1, self.channel, self.col_num, self.row_num).float()\n",
    "        )\n",
    "\n",
    "    def feature(self, state):\n",
    "        # 価値の計算\n",
    "        return self.teacher_model(\n",
    "            torch.from_numpy(state).view(-1, self.channel, self.col_num, self.row_num).float()\n",
    "        )\n",
    "\n",
    "    def policy(self, state, epsilon):\n",
    "        # 状態からCNNの出力に基づき、次の行動を選択\n",
    "        if np.random.random() < epsilon:\n",
    "            # 探索\n",
    "            return int(np.random.choice(\n",
    "                [c for c in range(len(self.actions)) if state.board[c] == 0]\n",
    "            ))\n",
    "        else:\n",
    "            # Actionの価値を取得\n",
    "            prediction = self.estimate(self.preprocess(state))[0].detach().numpy()\n",
    "            for i in range(len(self.actions)):\n",
    "                # ゲーム上選択可能なactionに絞る\n",
    "                if state.board[i] != 0:\n",
    "                    prediction[i] = -1e7\n",
    "            return int(np.argmax(prediction))\n",
    "\n",
    "    def update(self, gamma):\n",
    "        # 行動履歴が十分に蓄積されている\n",
    "        if len(self.experience['s']) < self.min_experiences:\n",
    "            return\n",
    "        # 行動履歴から学習用のデータのidをサンプリングする\n",
    "        ids = np.random.randint(low=0, high=len(self.experience['s']), size=32)\n",
    "        states = np.asarray([self.preprocess(self.experience['s'][i]) for i in ids])\n",
    "        states_next = np.asarray([self.preprocess(self.experience['n_s'][i]) for i in ids])\n",
    "        # 価値の計算\n",
    "        estimateds = self.estimate(states).detach().numpy()\n",
    "        feature = self.feature(states_next).detach().numpy()\n",
    "        target = estimateds.copy()\n",
    "        for idx, i in enumerate(ids):\n",
    "            a = self.experience['a'][i]\n",
    "            r = self.experience['r'][i]\n",
    "            d = self.experience['done'][i]\n",
    "\n",
    "            if d:\n",
    "                reward = r\n",
    "            else:\n",
    "                reward = r + gamma * np.max(feature[idx])\n",
    "        # TD誤差を小さくするようにCNNを更新\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterrion(\n",
    "            torch.tensor(estimateds, requires_grad=True),\n",
    "            torch.tensor(target, requires_grad=True)\n",
    "        )\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_teacher(self):\n",
    "        # 繊維先の価値の更新\n",
    "        self.teacher_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "\n",
    "class DeepQNetworkTrainer():\n",
    "    def __init__(self, env, epsilon=0.9):\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        self.agent = DeepQNetworkAgent(env)\n",
    "        self.reward_log = []\n",
    "        self.epsilon_log = []\n",
    "        self.num_column = env.configuration['columns']\n",
    "        self.num_row = env.configuration['rows']\n",
    "\n",
    "    def check_spot_pattern(self, state, pattern, check_mark, mode='v'):\n",
    "        if mode == 'v':\n",
    "            state = state\n",
    "        elif mode == 'h':\n",
    "            state = state.T\n",
    "\n",
    "        pattern = np.array([check_mark if p else 0 for p in pattern])\n",
    "\n",
    "        n_window = len(pattern)\n",
    "        n_window_list = np.array([\n",
    "            row[i:i + n_window] for row in state for i in range(len(row) - n_window + 1)\n",
    "        ])\n",
    "\n",
    "        num_filled = np.all(n_window_list == pattern, axis=1).sum()\n",
    "        return num_filled\n",
    "\n",
    "    def custom_reward(self, state, reward, done):\n",
    "        my_mark = state['mark']\n",
    "        enemy_mark = state['mark'] % 2 + 1\n",
    "\n",
    "        board = np.array(state['board']).reshape(self.num_column, self.num_row)\n",
    "\n",
    "        # Clipping\n",
    "        if done:\n",
    "            if reward == 1:  # 勝ち\n",
    "                return 10000\n",
    "            elif reward == 0:  # 負け\n",
    "                return -10000\n",
    "            else:  # 引き分け\n",
    "                return 5000\n",
    "        else:\n",
    "            score = -0.05\n",
    "            # Vertical\n",
    "            # Check Own Vertical win patterns\n",
    "            patterns = np.array([\n",
    "                [True, True, True, False],\n",
    "                [True, True, False, True],\n",
    "                [True, False, True, True],\n",
    "                [False, True, True, True],\n",
    "            ])\n",
    "            for pattern in patterns:\n",
    "                score += self.check_spot_pattern(board, pattern, my_mark, mode='v')\n",
    "            # Check Enemy Vertical win patterns\n",
    "            for pattern in patterns:\n",
    "                score -= 100 * self.check_spot_pattern(board, pattern, enemy_mark, mode='v')\n",
    "            # Horizontal\n",
    "            # Check Own Horizontal win patterns\n",
    "            pattern = np.array([False, True, True, True])\n",
    "            score += self.check_spot_pattern(board, pattern, my_mark, mode='h')\n",
    "            # Check Enemy Horizontal win patterns\n",
    "            score -= 100 * self.check_spot_pattern(board, pattern, enemy_mark, mode='h')\n",
    "\n",
    "            return score\n",
    "\n",
    "    def train(self, trainer, epsilon_decay_rate=0.9999,\n",
    "              min_epsilon=0.01, episode_cnt=100, gamma=0.6):\n",
    "        cnt = 0\n",
    "        for episode in tqdm(range(episode_cnt)):\n",
    "            rewards = []\n",
    "            state = trainer.reset()  # ゲーム環境リセット\n",
    "            self.epsilon = max(min_epsilon, self.epsilon * epsilon_decay_rate)\n",
    "\n",
    "            while not self.env.done:\n",
    "                # どの列にドロップするか決める\n",
    "                action = self.agent.policy(state, self.epsilon)\n",
    "                prev_state = state\n",
    "                state, reward, done, _ = trainer.step(action)\n",
    "                reward = self.custom_reward(state, reward, done)\n",
    "                # 行動履歴の蓄積\n",
    "                exp = {'s': prev_state, 'a': action, 'r': reward, 'n_s': state, 'done': done}\n",
    "                self.agent.add_experience(exp)\n",
    "                # 価値評価の更新\n",
    "                self.agent.update(gamma)\n",
    "                cnt += 1\n",
    "                if cnt % 100 == 0:\n",
    "                    # 遷移先価値計算用の更新\n",
    "                    self.agent.update_teacher()\n",
    "            self.reward_log.append(reward)\n",
    "            self.epsilon_log.append(self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../v01000/cache/v01002_dq_trainer.pkl'\n",
    "dq_trainer = load_pickle(model_path)\n",
    "\n",
    "def load_model():\n",
    "    model = CNN(7)\n",
    "#     encoded_weights = \"{model_state_dict_bin}\".encode()\n",
    "#     weights = pickle.loads(base64.b64decode(encoded_weights))\n",
    "    model.load_state_dict(dq_trainer.agent.model.state_dict())\n",
    "    return model\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "def agent(observation, config):\n",
    "    return random.choice([c for c in range(config.columns) if observation.board[c] == 0])\n",
    "    col_num = config.columns\n",
    "    row_num = config.rows\n",
    "    channel = 1\n",
    "\n",
    "    state = preprocess(observation, col_num, row_num)\n",
    "    prediction = model(\n",
    "        torch.from_numpy(state).view(-1, channel, col_num, row_num).float()\n",
    "    ).detach().numpy()\n",
    "    action = int(np.argmax(prediction))\n",
    "\n",
    "    # 選んだActionが、ゲーム上選べない場合\n",
    "    if observation.board[action] != 0:\n",
    "        return random.choice([c for c in range(config.columns) if observation.board[c] == 0])\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = kaggle_env.make(\"connectx\", debug=False)\n",
    "trainer = env.train([None, \"negamax\"])\n",
    "\n",
    "state = trainer.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5823d4ccbf02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_path' is not defined"
     ]
    }
   ],
   "source": [
    "def agent(observation, config):\n",
    "    return random.choice([c for c in range(config.columns) if observation.board[c] == 0])\n",
    "    col_num = config.columns\n",
    "    row_num = config.rows\n",
    "    channel = 1\n",
    "\n",
    "    state = preprocess(observation, col_num, row_num)\n",
    "    prediction = model(\n",
    "        torch.from_numpy(state).view(-1, channel, col_num, row_num).float()\n",
    "    ).detach().numpy()\n",
    "    action = int(np.argmax(prediction))\n",
    "\n",
    "    # 選んだActionが、ゲーム上選べない場合\n",
    "    if observation.board[action] != 0:\n",
    "        return random.choice([c for c in range(config.columns) if observation.board[c] == 0])\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
